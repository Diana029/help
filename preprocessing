##### 1.One hot encoding manually(top 20 + others)
#top_n -- сколько уникальных значений для категориальной переменной оставляем, а остальные заменяем на others
#ds - датасет
def one_hot_encoding(top_n,ds):
    #находим названия категориальных признаков
    cat_columns = ds.columns[ds.columns.str.contains('_cat')]
    #выделяем из них subset
    df_object = ds[cat_columns].copy()
    #находим кол-во уникальных значений для каждого категориального столбца
    cnt_nun_cat = df_object.apply(pd.Series.nunique)
    #находим те категориальные признаки, где уникальных значений слишком много
    list_long_cat = cnt_nun_cat[cnt_nun_cat > top_n]
    #находим названия этих столбцов
    list_long_cat_indx = list_long_cat.index
    #для этих столбцов делаем следующее:
    for i in list_long_cat_indx:
        #находим чаще встречающиеся значения в этих столбцах(сами отсортированы по убыванию)
        vals = ds[i].value_counts()
        #обрезаем top_n сверху уникальных значений
        vals_head = vals.head(top_n)
        #остальные все значения заменяем на 'others'
        ds.loc[~ds[i].isin(vals_head.index.tolist()), i] = 'others'
    #делаем one hot encoding
    df_object_1 = ds[cat_columns].copy()
    df_cat = pd.get_dummies(df_object_1, drop_first= True)
    #находим некатегориальные признаки
    not_cat_columns = ds.columns[~ds.columns.str.contains('_cat')]
    result = pd.concat([ds[not_cat_columns], df_cat], axis=1)
    return result


df = one_hot_encoding(20, dataset)

##### 2.One hot encoding basical
df_cat = pd.get_dummies(df_object_1, drop_first= True)

##### 3.replace NaN values
df = df.fillna(data.mean())

from sklearn.preprocessing import Imputer
imp = Imputer(missing_values='NaN', strategy='most_frequent', axis=1)
# Impute our data, then train
df_a2_imp = imp.fit_transform(df_a2)
imp1 = Imputer(missing_values='NaN', strategy='median', axis=1)
df_a4 = df_a4.fillna(-1)


##### 4.preprocess missing values
from sklearn.base import TransformerMixin
class DataFrameImputer(TransformerMixin):
    def __init__(self):
        """Impute missing values.
        Columns of dtype object are imputed with the most frequent value 
        in column.
        Columns of other types are imputed with mean of column.
        """
    def fit(self, X, y=None):
        self.fill = pd.Series([X[c].value_counts().index[0]
            if X[c].dtype == np.dtype('O') else X[c].mean() for c in X],
            index=X.columns)
        return self
    def transform(self, X, y=None):
        return X.fillna(self.fill)

##### 5. Preprocessing categorical features by catboost
categorical_feats = [
    f for f in df.columns if df[f].dtype == 'object'
]
for header in categorical_feats:
    if header!= 'inn' and header!= 'id_date' and header!= 'LABEL':
        df[header] = df[header].astype('category').cat.codes

##### 6. Check and Remove Constant Features
df = data.loc[:,data.apply(pd.Series.nunique) != 1]

##### 7. Смотрим есть ли фичи с NearZeroVariance c порогом (берём только те, у которых в столбцах не одинаковые значения)
from sklearn.feature_selection import VarianceThreshold
def variance_threshold_selector(data, threshold):
    selector = VarianceThreshold(threshold)
    selector.fit(data)
    return data[data.columns[selector.get_support(indices=True)]]
X_tr = variance_threshold_selector(X, 0.0)


##### 6. Снижение размерности by max объяснённой variance
from numpy.linalg import eig
for f in df.columns:
    if df[f].dtype == 'object':
        print(f)
X_1 = X.ix[:, X.columns != "inn"]
#Estimate a covariance matrix, given data and weights.
covX = np.cov(X_1.T)
eigs, vecs = eig(covX)
eig_pairs = [(abs(eigs[i]),vecs[:,i]) for i in range(len(eigs))]
eig_pairs.sort(key = lambda x: x[0], reverse= True)
E = sum(eigs)
exp_variance = [(i/E)*100 for i in sorted(eigs, reverse=True)] # Individual explained variance
cum_variance = np.cumsum(exp_variance) # Cumulative explained variance
plt.figure(figsize=(10,7))
plt.bar(range(len(eigs)), exp_variance, alpha=0.8, label='Variance of individual PC, %', color = 'r')
plt.step(range(len(eigs)), cum_variance, label='Cumulative variance, %')
plt.ylabel('Total variance %')
plt.xlabel('Number of principal components')
plt.xlim(0,7)
plt.legend()
plt.show()
